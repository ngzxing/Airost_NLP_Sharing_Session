{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9609c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split() ])\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "                \n",
    "def remove_numbers(text):\n",
    "                    \n",
    "    return \" \".join([word for word in text.split() if not word.isdigit()])\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    \n",
    "    text = re.sub(\"[%s]\" % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), \" \", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "                    \n",
    "def lower_case(text):\n",
    "                    \n",
    "    return \" \".join([word.casefold() for word in text.split()])\n",
    "                    \n",
    "\n",
    "def normalize_text(data):\n",
    "    \n",
    "    data.text = data.text.apply(lambda text: lower_case(text))\n",
    "    data.text = data.text.apply(lambda text: remove_stop_words(text))\n",
    "    data.text = data.text.apply(lambda text: remove_numbers(text))\n",
    "    data.text = data.text.apply(lambda text: remove_punctuations(text))\n",
    "    data.text = data.text.apply(lambda text: lemmatization(text))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a437c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gloves():\n",
    "    \n",
    "    word_to_vec= dict()\n",
    "    word_to_index = dict()\n",
    "    index_to_word = dict()\n",
    "    \n",
    "    with open(\"glove.6b.300d.txt\", \"r\",encoding='utf-8') as f:\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            data = line.strip().split()\n",
    "            word_to_vec[data[0]] = np.array(data[1:], dtype = np.float64)\n",
    "            word_to_index[data[0]] = i+1;\n",
    "            index_to_word[i+1] = data[0]\n",
    "            \n",
    "    return word_to_vec, word_to_index, index_to_word\n",
    "\n",
    "def get_word_beg(X_train):\n",
    "    \n",
    "    wordBag = set()\n",
    "    \n",
    "    for x in X_train:\n",
    "        for word in x.split():\n",
    "            wordBag.add(word.lower())\n",
    "            \n",
    "    for x in X_test:\n",
    "        for word in x.split():\n",
    "            wordBag.add(word.lower())\n",
    "    \n",
    "    wordBag.add(\"unk\")\n",
    "    \n",
    "    word_to_index = dict()\n",
    "    index_to_word = dict()\n",
    "    for i, word in enumerate(wordBag):\n",
    "        word_to_index[word] = i+1\n",
    "        index_to_word[i+1] = word\n",
    "        \n",
    "    return  word_to_index, index_to_word\n",
    "    \n",
    "    \n",
    "def x_to_indices(X, maxLen, word_to_indices):\n",
    "    \n",
    "    X_indices = np.zeros( (X.shape[0], maxLen) )\n",
    "    \n",
    "    for i,x in enumerate(X):\n",
    "        for j, word in zip(range( min(maxLen,len(x.split())) ), x.split()):\n",
    "            \n",
    "            if word.lower() in word_to_indices:\n",
    "                X_indices[i, j] = word_to_indices[word.lower()]\n",
    "            else:\n",
    "                X_indices[i, j] = word_to_indices[\"unk\"]\n",
    "            \n",
    "    return X_indices.astype(int)\n",
    "\n",
    "\n",
    "def one_hot_x(X_indices, word_to_index):\n",
    "    \n",
    "    I = np.eye( len(word_to_index) + 1 )\n",
    "    I[0,0] = 0\n",
    "    return I[X_indices]\n",
    "        \n",
    "        \n",
    "def one_hot_y(Y, cls = 5):\n",
    "    \n",
    "    return np.eye(cls)[Y]\n",
    "\n",
    "\n",
    "def get_label():\n",
    "    \n",
    "    return { 0: \"sadness\", 1 : \"joy\", 2 : \"love\", 3: \"anger\", 4 : \"fear\", 5 : \"surprise\"}\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
